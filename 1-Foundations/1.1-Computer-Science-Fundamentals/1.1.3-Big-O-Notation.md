# 1.1.3 Big O Notation

Analyzing time and space complexity to predict how code performs at scale.

---

## What Is Big O Notation?

**Big O notation** describes the upper bound of an algorithm's growth rate. It tells you how the runtime or memory usage scales as the input size increases.

Big O answers: *"If I double my input size, how much longer will this take?"*

---

## Why Big O Matters

| Input Size | O(1) | O(log n) | O(n) | O(n log n) | O(n²) | O(2ⁿ) |
|------------|------|----------|------|------------|-------|-------|
| 10 | 1 | 3 | 10 | 33 | 100 | 1,024 |
| 100 | 1 | 7 | 100 | 664 | 10,000 | 10³⁰ |
| 1,000 | 1 | 10 | 1,000 | 9,966 | 1,000,000 | 10³⁰⁰ |
| 1,000,000 | 1 | 20 | 1,000,000 | 20,000,000 | 10¹² | ∞ |

The difference between O(n) and O(n²) can mean **seconds vs. hours** at scale.

---

## Common Time Complexities

### O(1) - Constant Time
Operations that take the same time regardless of input size.

```python
def get_first(arr):
    return arr[0]  # O(1)

def get_by_key(dictionary, key):
    return dictionary[key]  # O(1) average

def is_even(n):
    return n % 2 == 0  # O(1)
```

**Examples:**
- Array index access
- Hash table lookup
- Push/pop on a stack
- Basic arithmetic operations

---

### O(log n) - Logarithmic Time
Halving the problem size each step. Very efficient.

```python
def binary_search(arr, target):
    left, right = 0, len(arr) - 1

    while left <= right:  # Runs log₂(n) times
        mid = (left + right) // 2
        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1

    return -1
```

**Examples:**
- Binary search
- Operations on balanced BST
- Finding power using exponentiation by squaring

**Why log n?**
```
n = 16: 16 → 8 → 4 → 2 → 1 (4 steps = log₂(16))
n = 1024: 1024 → 512 → ... → 1 (10 steps = log₂(1024))
```

---

### O(n) - Linear Time
Processing each element once.

```python
def find_max(arr):
    max_val = arr[0]
    for num in arr:  # n iterations
        if num > max_val:
            max_val = num
    return max_val

def sum_array(arr):
    total = 0
    for num in arr:  # n iterations
        total += num
    return total
```

**Examples:**
- Linear search
- Finding min/max
- Traversing a linked list
- Counting elements

---

### O(n log n) - Linearithmic Time
Efficient sorting algorithms and divide-and-conquer.

```python
def merge_sort(arr):
    if len(arr) <= 1:
        return arr

    mid = len(arr) // 2
    left = merge_sort(arr[:mid])   # T(n/2)
    right = merge_sort(arr[mid:])  # T(n/2)

    return merge(left, right)      # O(n) merge

# Total: O(n log n)
# - We divide log n times
# - Each level does O(n) work merging
```

**Examples:**
- Merge sort
- Quick sort (average case)
- Heap sort
- Many divide-and-conquer algorithms

---

### O(n²) - Quadratic Time
Nested loops over the input.

```python
def bubble_sort(arr):
    n = len(arr)
    for i in range(n):            # n iterations
        for j in range(n - 1):    # n iterations
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
    return arr

def find_pairs(arr):
    """Find all pairs - O(n²)"""
    pairs = []
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            pairs.append((arr[i], arr[j]))
    return pairs
```

**Examples:**
- Bubble sort, selection sort, insertion sort
- Checking all pairs
- Naive string matching
- Some dynamic programming solutions

---

### O(n³) - Cubic Time
Triple nested loops.

```python
def matrix_multiply(A, B):
    """Naive matrix multiplication - O(n³)"""
    n = len(A)
    result = [[0] * n for _ in range(n)]

    for i in range(n):
        for j in range(n):
            for k in range(n):
                result[i][j] += A[i][k] * B[k][j]

    return result
```

---

### O(2ⁿ) - Exponential Time
Doubles with each additional input element.

```python
def fibonacci_naive(n):
    """O(2ⁿ) - very slow!"""
    if n <= 1:
        return n
    return fibonacci_naive(n - 1) + fibonacci_naive(n - 2)

def all_subsets(arr):
    """Generate power set - O(2ⁿ)"""
    if len(arr) == 0:
        return [[]]

    first = arr[0]
    rest_subsets = all_subsets(arr[1:])

    return rest_subsets + [[first] + s for s in rest_subsets]
```

**Examples:**
- Naive Fibonacci
- Power set generation
- Some brute-force solutions
- Solving NP-complete problems

---

### O(n!) - Factorial Time
Extremely slow—avoid if possible.

```python
def permutations(arr):
    """Generate all permutations - O(n!)"""
    if len(arr) <= 1:
        return [arr]

    result = []
    for i in range(len(arr)):
        rest = arr[:i] + arr[i+1:]
        for p in permutations(rest):
            result.append([arr[i]] + p)

    return result

# n=10: 3,628,800 permutations
# n=20: 2,432,902,008,176,640,000 permutations
```

**Examples:**
- Generating all permutations
- Brute-force traveling salesman
- Some combinatorial problems

---

## Visual Comparison

```
Time
  ↑
  │                                          n!
  │                                       /
  │                                     /
  │                                   /
  │                              2ⁿ /
  │                            /
  │                         /
  │                     n³/
  │                   /
  │               n²/
  │             /
  │        n log n
  │       /
  │     n
  │   /
  │  log n
  │ ────────────────────────────── 1
  └─────────────────────────────────→ Input Size (n)
```

---

## How to Analyze Code

### Rule 1: Drop Constants
O(2n) → O(n)
O(500) → O(1)

```python
def example(arr):
    for i in arr:      # O(n)
        print(i)

    for i in arr:      # O(n)
        print(i * 2)

# Total: O(n) + O(n) = O(2n) = O(n)
```

### Rule 2: Drop Lower Order Terms
O(n² + n) → O(n²)
O(n + log n) → O(n)

```python
def example(arr):
    # O(n²)
    for i in arr:
        for j in arr:
            print(i, j)

    # O(n)
    for i in arr:
        print(i)

# Total: O(n²) + O(n) = O(n²)
```

### Rule 3: Different Inputs = Different Variables
```python
def process(arr1, arr2):
    for i in arr1:       # O(a)
        print(i)

    for j in arr2:       # O(b)
        print(j)

# Total: O(a + b), NOT O(n)

def intersect(arr1, arr2):
    for i in arr1:           # O(a)
        for j in arr2:       # O(b)
            if i == j:
                print(i)

# Total: O(a * b)
```

### Rule 4: Analyze Loops Carefully

```python
# Not always O(n)!
def example(n):
    i = 1
    while i < n:
        print(i)
        i *= 2  # Doubles each time!

# Iterations: 1, 2, 4, 8, 16, ... n
# = O(log n)
```

---

## Complexity Analysis Examples

### Example 1: Simple Loop
```python
def sum_array(arr):
    total = 0           # O(1)
    for num in arr:     # O(n)
        total += num    # O(1)
    return total        # O(1)

# Total: O(1) + O(n) * O(1) + O(1) = O(n)
```

### Example 2: Nested Loops
```python
def print_pairs(arr):
    for i in range(len(arr)):        # O(n)
        for j in range(len(arr)):    # O(n)
            print(arr[i], arr[j])    # O(1)

# Total: O(n) * O(n) * O(1) = O(n²)
```

### Example 3: Tricky Nested Loop
```python
def print_pairs_once(arr):
    for i in range(len(arr)):        # O(n)
        for j in range(i, len(arr)): # O(n-i)
            print(arr[i], arr[j])

# Iterations: n + (n-1) + (n-2) + ... + 1
# = n(n+1)/2
# = O(n²)  (still quadratic!)
```

### Example 4: Loop with Early Exit
```python
def find_element(arr, target):
    for i, val in enumerate(arr):  # O(n) worst case
        if val == target:
            return i
    return -1

# Best case: O(1) - found at first position
# Worst case: O(n) - not found or at last position
# Average case: O(n)
```

### Example 5: Multiple Sequential Loops
```python
def complex_function(arr):
    # First loop: O(n)
    for i in arr:
        print(i)

    # Second loop: O(n²)
    for i in arr:
        for j in arr:
            print(i + j)

    # Third loop: O(n)
    for i in arr:
        print(i * 2)

# Total: O(n) + O(n²) + O(n) = O(n²)
```

### Example 6: Logarithmic Inside Linear
```python
def search_all(arr, targets):
    arr.sort()  # O(n log n)

    results = []
    for target in targets:                    # O(m) where m = len(targets)
        idx = binary_search(arr, target)      # O(log n)
        results.append(idx)

    return results

# Total: O(n log n) + O(m log n)
# If m ≈ n: O(n log n)
```

---

## Space Complexity

Space complexity measures memory usage as input grows.

### O(1) Space - Constant
```python
def find_max(arr):
    max_val = arr[0]  # Single variable
    for num in arr:
        max_val = max(max_val, num)
    return max_val
```

### O(n) Space - Linear
```python
def duplicate_array(arr):
    return arr[:]  # Creates copy of size n

def collect_evens(arr):
    result = []  # Could grow to size n
    for num in arr:
        if num % 2 == 0:
            result.append(num)
    return result
```

### O(n) Space - Recursion Stack
```python
def factorial(n):
    if n <= 1:
        return 1
    return n * factorial(n - 1)

# Call stack grows to depth n
# Space: O(n)
```

### Space Complexity Examples

| Algorithm | Time | Space |
|-----------|------|-------|
| Binary search (iterative) | O(log n) | O(1) |
| Binary search (recursive) | O(log n) | O(log n) |
| Merge sort | O(n log n) | O(n) |
| Quick sort | O(n log n) | O(log n) |
| Heap sort | O(n log n) | O(1) |
| BFS | O(V + E) | O(V) |
| DFS (recursive) | O(V + E) | O(V) |

---

## Amortized Analysis

Sometimes individual operations vary, but average out over time.

### Dynamic Array (ArrayList)

```python
class DynamicArray:
    def __init__(self):
        self.arr = [None] * 1
        self.size = 0
        self.capacity = 1

    def append(self, item):
        if self.size == self.capacity:
            # Resize: O(n)
            self._resize(2 * self.capacity)

        self.arr[self.size] = item  # O(1)
        self.size += 1

    def _resize(self, new_capacity):
        new_arr = [None] * new_capacity
        for i in range(self.size):
            new_arr[i] = self.arr[i]
        self.arr = new_arr
        self.capacity = new_capacity
```

**Analysis:**
- Most appends: O(1)
- Occasional resize: O(n)
- Amortized: O(1) per append

**Why?** If we start with capacity 1 and double:
- Insert 1: no resize
- Insert 2: resize (copy 1)
- Insert 3: no resize
- Insert 4: resize (copy 2)
- Insert 5: no resize
- ...

Total copies for n insertions: 1 + 2 + 4 + ... + n/2 ≈ n
Average per insertion: n/n = O(1)

---

## Best, Average, and Worst Case

### Quick Sort Example

```python
def quick_sort(arr, low, high):
    if low < high:
        pivot_idx = partition(arr, low, high)
        quick_sort(arr, low, pivot_idx - 1)
        quick_sort(arr, pivot_idx + 1, high)
```

| Case | When | Complexity |
|------|------|------------|
| **Best** | Pivot always splits array in half | O(n log n) |
| **Average** | Random pivot selection | O(n log n) |
| **Worst** | Pivot is always min/max (sorted input) | O(n²) |

### Binary Search Tree

| Case | When | Complexity |
|------|------|------------|
| **Best** | Balanced tree | O(log n) |
| **Average** | Randomly built tree | O(log n) |
| **Worst** | Completely unbalanced (like linked list) | O(n) |

---

## Common Complexity by Operation

### Array Operations
| Operation | Time |
|-----------|------|
| Access by index | O(1) |
| Search | O(n) |
| Insert at end | O(1)* |
| Insert at beginning | O(n) |
| Delete | O(n) |

*Amortized for dynamic arrays

### Hash Table Operations
| Operation | Average | Worst |
|-----------|---------|-------|
| Search | O(1) | O(n) |
| Insert | O(1) | O(n) |
| Delete | O(1) | O(n) |

### Tree Operations (Balanced)
| Operation | Time |
|-----------|------|
| Search | O(log n) |
| Insert | O(log n) |
| Delete | O(log n) |
| Traverse | O(n) |

---

## Optimization Strategies

### 1. Trade Space for Time
```python
# O(n²) - checking all pairs
def has_duplicate_slow(arr):
    for i in range(len(arr)):
        for j in range(i + 1, len(arr)):
            if arr[i] == arr[j]:
                return True
    return False

# O(n) - using extra space
def has_duplicate_fast(arr):
    seen = set()  # O(n) space
    for num in arr:
        if num in seen:
            return True
        seen.add(num)
    return False
```

### 2. Precompute Values
```python
# Slow: O(q * n) for q queries
def range_sum_slow(arr, queries):
    results = []
    for l, r in queries:
        results.append(sum(arr[l:r+1]))  # O(n) each
    return results

# Fast: O(n) precompute + O(q) queries = O(n + q)
def range_sum_fast(arr, queries):
    # Build prefix sum - O(n)
    prefix = [0]
    for num in arr:
        prefix.append(prefix[-1] + num)

    # Answer queries - O(1) each
    results = []
    for l, r in queries:
        results.append(prefix[r+1] - prefix[l])
    return results
```

### 3. Use Better Data Structures
```python
# O(n) search with list
def find_in_list(lst, target):
    return target in lst  # O(n)

# O(1) search with set
def find_in_set(s, target):
    return target in s  # O(1)
```

---

## Interview Tips

1. **Always state complexity**: "This solution is O(n log n) time and O(n) space"

2. **Discuss trade-offs**: "We can do O(n²) with O(1) space, or O(n) with O(n) space"

3. **Consider all cases**: "Best case is O(1), average and worst are O(n)"

4. **Know your constants**: O(n) with large constants can be slower than O(n log n) for small n

5. **Watch for hidden complexity**:
   ```python
   for i in range(n):
       arr.insert(0, i)  # O(n) each!
   # Total: O(n²), not O(n)
   ```

---

## Quick Reference Chart

```
Complexity    |  Name          |  Example
--------------|----------------|---------------------------
O(1)          |  Constant      |  Array access, hash lookup
O(log n)      |  Logarithmic   |  Binary search
O(n)          |  Linear        |  Linear search, single loop
O(n log n)    |  Linearithmic  |  Merge sort, quick sort
O(n²)         |  Quadratic     |  Bubble sort, nested loops
O(n³)         |  Cubic         |  Matrix multiplication
O(2ⁿ)         |  Exponential   |  Subsets, naive Fibonacci
O(n!)         |  Factorial     |  Permutations
```

---

## Practice Problems

1. Analyze the time complexity of these functions:
   ```python
   def mystery1(n):
       for i in range(n):
           for j in range(n):
               for k in range(n):
                   print(i, j, k)

   def mystery2(n):
       i = n
       while i > 0:
           print(i)
           i //= 2

   def mystery3(arr):
       for i in range(len(arr)):
           arr.pop(0)
   ```

2. Which is faster for n = 1000: O(n²) algorithm that does 1 operation per iteration, or O(n log n) algorithm that does 100 operations per iteration?

3. What's the space complexity of recursive Fibonacci? How can you optimize it?

---

## Key Takeaways

1. **Big O describes growth rate**, not exact speed
2. **Drop constants and lower-order terms** when simplifying
3. **Different inputs need different variables** (O(a + b), not O(2n))
4. **Space matters too**—don't forget to analyze memory
5. **Amortized analysis** averages cost over many operations
6. **Trade-offs exist**: faster often means more memory
7. **Know common complexities** for standard data structures and algorithms
8. **Practice analyzing code**—it becomes intuitive with experience

---

*"Premature optimization is the root of all evil. But knowing complexity helps you avoid pessimization."*
