# 1.1.2 Algorithms

Sorting, searching, recursion, and dynamic programming—systematic approaches to solving problems.

---

## What Is an Algorithm?

An **algorithm** is a step-by-step procedure for solving a problem or accomplishing a task. It's like a recipe: a specific sequence of instructions that, when followed, produces a desired result.

---

## Why Algorithms Matter

| Reason | Explanation |
|--------|-------------|
| **Efficiency** | The difference between O(n²) and O(n log n) can mean seconds vs. hours |
| **Scalability** | Good algorithms handle growth; bad ones crash under load |
| **Problem Solving** | Recognizing patterns lets you apply known solutions |
| **Interviews** | Algorithm knowledge is heavily tested in technical interviews |

---

## Algorithm Paradigms

### 1. Brute Force
Try all possibilities until you find a solution.

```python
def find_pair_with_sum_brute(arr, target):
    """Find two numbers that add up to target - O(n²)"""
    n = len(arr)
    for i in range(n):
        for j in range(i + 1, n):
            if arr[i] + arr[j] == target:
                return (arr[i], arr[j])
    return None

# Better: Use hash map - O(n)
def find_pair_with_sum(arr, target):
    seen = set()
    for num in arr:
        complement = target - num
        if complement in seen:
            return (complement, num)
        seen.add(num)
    return None
```

### 2. Divide and Conquer
Break problem into smaller subproblems, solve them, and combine results.

```python
def merge_sort(arr):
    """Classic divide and conquer - O(n log n)"""
    if len(arr) <= 1:
        return arr

    # Divide
    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])

    # Conquer (merge)
    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0

    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1

    result.extend(left[i:])
    result.extend(right[j:])
    return result
```

### 3. Greedy
Make the locally optimal choice at each step.

```python
def coin_change_greedy(coins, amount):
    """
    Greedy doesn't always work for coin change!
    Works for standard US coins but not all coin systems.
    """
    coins = sorted(coins, reverse=True)
    result = []

    for coin in coins:
        while amount >= coin:
            result.append(coin)
            amount -= coin

    return result if amount == 0 else None

# Example where greedy fails:
# coins = [1, 3, 4], amount = 6
# Greedy: [4, 1, 1] = 3 coins
# Optimal: [3, 3] = 2 coins
```

### 4. Dynamic Programming
Store solutions to subproblems to avoid recomputation.

```python
def fibonacci_dp(n):
    """Bottom-up DP - O(n) time, O(n) space"""
    if n <= 1:
        return n

    dp = [0] * (n + 1)
    dp[1] = 1

    for i in range(2, n + 1):
        dp[i] = dp[i-1] + dp[i-2]

    return dp[n]

def fibonacci_optimized(n):
    """Space-optimized - O(n) time, O(1) space"""
    if n <= 1:
        return n

    prev2, prev1 = 0, 1
    for _ in range(2, n + 1):
        curr = prev1 + prev2
        prev2, prev1 = prev1, curr

    return prev1
```

### 5. Backtracking
Try solutions incrementally, abandoning paths that can't lead to valid solutions.

```python
def permutations(nums):
    """Generate all permutations using backtracking"""
    result = []

    def backtrack(path, remaining):
        if not remaining:
            result.append(path[:])
            return

        for i in range(len(remaining)):
            path.append(remaining[i])
            backtrack(path, remaining[:i] + remaining[i+1:])
            path.pop()  # Backtrack

    backtrack([], nums)
    return result

# permutations([1, 2, 3])
# [[1,2,3], [1,3,2], [2,1,3], [2,3,1], [3,1,2], [3,2,1]]
```

---

## Sorting Algorithms

### Comparison of Sorting Algorithms

| Algorithm | Best | Average | Worst | Space | Stable |
|-----------|------|---------|-------|-------|--------|
| Bubble Sort | O(n) | O(n²) | O(n²) | O(1) | Yes |
| Selection Sort | O(n²) | O(n²) | O(n²) | O(1) | No |
| Insertion Sort | O(n) | O(n²) | O(n²) | O(1) | Yes |
| Merge Sort | O(n log n) | O(n log n) | O(n log n) | O(n) | Yes |
| Quick Sort | O(n log n) | O(n log n) | O(n²) | O(log n) | No |
| Heap Sort | O(n log n) | O(n log n) | O(n log n) | O(1) | No |

### Bubble Sort
Simple but inefficient—good for learning, never for production.

```python
def bubble_sort(arr):
    """O(n²) - swap adjacent elements if out of order"""
    n = len(arr)
    for i in range(n):
        swapped = False
        for j in range(0, n - i - 1):
            if arr[j] > arr[j + 1]:
                arr[j], arr[j + 1] = arr[j + 1], arr[j]
                swapped = True
        if not swapped:  # Optimization: already sorted
            break
    return arr
```

### Selection Sort
Find minimum, put at front, repeat.

```python
def selection_sort(arr):
    """O(n²) - find minimum, place at correct position"""
    n = len(arr)
    for i in range(n):
        min_idx = i
        for j in range(i + 1, n):
            if arr[j] < arr[min_idx]:
                min_idx = j
        arr[i], arr[min_idx] = arr[min_idx], arr[i]
    return arr
```

### Insertion Sort
Build sorted array one element at a time. Great for small or nearly-sorted data.

```python
def insertion_sort(arr):
    """O(n²) worst, O(n) best - insert each element in sorted position"""
    for i in range(1, len(arr)):
        key = arr[i]
        j = i - 1
        while j >= 0 and arr[j] > key:
            arr[j + 1] = arr[j]
            j -= 1
        arr[j + 1] = key
    return arr
```

### Merge Sort
Divide and conquer—guaranteed O(n log n).

```python
def merge_sort(arr):
    """O(n log n) - divide, sort, merge"""
    if len(arr) <= 1:
        return arr

    mid = len(arr) // 2
    left = merge_sort(arr[:mid])
    right = merge_sort(arr[mid:])

    return merge(left, right)

def merge(left, right):
    result = []
    i = j = 0

    while i < len(left) and j < len(right):
        if left[i] <= right[j]:
            result.append(left[i])
            i += 1
        else:
            result.append(right[j])
            j += 1

    result.extend(left[i:])
    result.extend(right[j:])
    return result
```

### Quick Sort
Fast in practice, but O(n²) worst case with bad pivot selection.

```python
def quick_sort(arr):
    """O(n log n) average - partition around pivot"""
    if len(arr) <= 1:
        return arr

    pivot = arr[len(arr) // 2]  # Better pivot selection
    left = [x for x in arr if x < pivot]
    middle = [x for x in arr if x == pivot]
    right = [x for x in arr if x > pivot]

    return quick_sort(left) + middle + quick_sort(right)

# In-place version (more memory efficient)
def quick_sort_inplace(arr, low=0, high=None):
    if high is None:
        high = len(arr) - 1

    if low < high:
        pivot_idx = partition(arr, low, high)
        quick_sort_inplace(arr, low, pivot_idx - 1)
        quick_sort_inplace(arr, pivot_idx + 1, high)

    return arr

def partition(arr, low, high):
    pivot = arr[high]
    i = low - 1

    for j in range(low, high):
        if arr[j] <= pivot:
            i += 1
            arr[i], arr[j] = arr[j], arr[i]

    arr[i + 1], arr[high] = arr[high], arr[i + 1]
    return i + 1
```

### Heap Sort
Uses heap data structure—guaranteed O(n log n) with O(1) extra space.

```python
def heap_sort(arr):
    """O(n log n) - build max heap, extract max repeatedly"""
    n = len(arr)

    # Build max heap
    for i in range(n // 2 - 1, -1, -1):
        heapify(arr, n, i)

    # Extract elements one by one
    for i in range(n - 1, 0, -1):
        arr[0], arr[i] = arr[i], arr[0]
        heapify(arr, i, 0)

    return arr

def heapify(arr, n, i):
    largest = i
    left = 2 * i + 1
    right = 2 * i + 2

    if left < n and arr[left] > arr[largest]:
        largest = left

    if right < n and arr[right] > arr[largest]:
        largest = right

    if largest != i:
        arr[i], arr[largest] = arr[largest], arr[i]
        heapify(arr, n, largest)
```

---

## Searching Algorithms

### Linear Search
Check each element one by one. Works on unsorted data.

```python
def linear_search(arr, target):
    """O(n) - check each element"""
    for i, val in enumerate(arr):
        if val == target:
            return i
    return -1
```

### Binary Search
Divide search space in half each time. **Requires sorted data.**

```python
def binary_search(arr, target):
    """O(log n) - divide and conquer on sorted array"""
    left, right = 0, len(arr) - 1

    while left <= right:
        mid = (left + right) // 2

        if arr[mid] == target:
            return mid
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1

    return -1

# Recursive version
def binary_search_recursive(arr, target, left=0, right=None):
    if right is None:
        right = len(arr) - 1

    if left > right:
        return -1

    mid = (left + right) // 2

    if arr[mid] == target:
        return mid
    elif arr[mid] < target:
        return binary_search_recursive(arr, target, mid + 1, right)
    else:
        return binary_search_recursive(arr, target, left, mid - 1)
```

### Binary Search Variations

```python
def find_first_occurrence(arr, target):
    """Find first index of target in sorted array with duplicates"""
    left, right = 0, len(arr) - 1
    result = -1

    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            result = mid
            right = mid - 1  # Keep searching left
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1

    return result

def find_last_occurrence(arr, target):
    """Find last index of target in sorted array with duplicates"""
    left, right = 0, len(arr) - 1
    result = -1

    while left <= right:
        mid = (left + right) // 2
        if arr[mid] == target:
            result = mid
            left = mid + 1  # Keep searching right
        elif arr[mid] < target:
            left = mid + 1
        else:
            right = mid - 1

    return result

def search_rotated_array(arr, target):
    """Search in rotated sorted array - O(log n)"""
    left, right = 0, len(arr) - 1

    while left <= right:
        mid = (left + right) // 2

        if arr[mid] == target:
            return mid

        # Left half is sorted
        if arr[left] <= arr[mid]:
            if arr[left] <= target < arr[mid]:
                right = mid - 1
            else:
                left = mid + 1
        # Right half is sorted
        else:
            if arr[mid] < target <= arr[right]:
                left = mid + 1
            else:
                right = mid - 1

    return -1
```

---

## Recursion

### Understanding Recursion

Every recursive function needs:
1. **Base case**: When to stop
2. **Recursive case**: How to break down the problem

```python
def factorial(n):
    """Classic recursion example"""
    # Base case
    if n <= 1:
        return 1
    # Recursive case
    return n * factorial(n - 1)

# factorial(5)
# = 5 * factorial(4)
# = 5 * 4 * factorial(3)
# = 5 * 4 * 3 * factorial(2)
# = 5 * 4 * 3 * 2 * factorial(1)
# = 5 * 4 * 3 * 2 * 1
# = 120
```

### Recursion vs Iteration

```python
# Recursive sum
def sum_recursive(arr):
    if len(arr) == 0:
        return 0
    return arr[0] + sum_recursive(arr[1:])

# Iterative sum (usually preferred)
def sum_iterative(arr):
    total = 0
    for num in arr:
        total += num
    return total
```

### Common Recursive Patterns

```python
# 1. Array traversal
def find_max(arr, idx=0, current_max=float('-inf')):
    if idx == len(arr):
        return current_max
    return find_max(arr, idx + 1, max(current_max, arr[idx]))

# 2. String processing
def reverse_string(s):
    if len(s) <= 1:
        return s
    return reverse_string(s[1:]) + s[0]

# 3. Tree traversal (most natural use of recursion)
def tree_sum(node):
    if node is None:
        return 0
    return node.value + tree_sum(node.left) + tree_sum(node.right)

# 4. Generate all subsets (power set)
def subsets(nums):
    result = []

    def backtrack(start, path):
        result.append(path[:])
        for i in range(start, len(nums)):
            path.append(nums[i])
            backtrack(i + 1, path)
            path.pop()

    backtrack(0, [])
    return result
```

---

## Dynamic Programming

### When to Use DP

DP is applicable when a problem has:
1. **Overlapping subproblems**: Same subproblems are solved multiple times
2. **Optimal substructure**: Optimal solution can be built from optimal solutions of subproblems

### Top-Down (Memoization)
Start with the original problem and break down, caching results.

```python
def fib_memoization(n, memo={}):
    if n in memo:
        return memo[n]
    if n <= 1:
        return n

    memo[n] = fib_memoization(n-1, memo) + fib_memoization(n-2, memo)
    return memo[n]
```

### Bottom-Up (Tabulation)
Start with smallest subproblems and build up.

```python
def fib_tabulation(n):
    if n <= 1:
        return n

    dp = [0] * (n + 1)
    dp[1] = 1

    for i in range(2, n + 1):
        dp[i] = dp[i-1] + dp[i-2]

    return dp[n]
```

### Classic DP Problems

#### 1. Climbing Stairs
```python
def climb_stairs(n):
    """
    Ways to climb n stairs, taking 1 or 2 steps at a time.
    Same as Fibonacci!
    """
    if n <= 2:
        return n

    dp = [0] * (n + 1)
    dp[1], dp[2] = 1, 2

    for i in range(3, n + 1):
        dp[i] = dp[i-1] + dp[i-2]

    return dp[n]
```

#### 2. Coin Change
```python
def coin_change(coins, amount):
    """Minimum coins needed to make amount"""
    dp = [float('inf')] * (amount + 1)
    dp[0] = 0

    for coin in coins:
        for x in range(coin, amount + 1):
            dp[x] = min(dp[x], dp[x - coin] + 1)

    return dp[amount] if dp[amount] != float('inf') else -1
```

#### 3. Longest Common Subsequence
```python
def lcs(text1, text2):
    """Length of longest common subsequence"""
    m, n = len(text1), len(text2)
    dp = [[0] * (n + 1) for _ in range(m + 1)]

    for i in range(1, m + 1):
        for j in range(1, n + 1):
            if text1[i-1] == text2[j-1]:
                dp[i][j] = dp[i-1][j-1] + 1
            else:
                dp[i][j] = max(dp[i-1][j], dp[i][j-1])

    return dp[m][n]
```

#### 4. 0/1 Knapsack
```python
def knapsack(weights, values, capacity):
    """Maximum value that fits in knapsack"""
    n = len(weights)
    dp = [[0] * (capacity + 1) for _ in range(n + 1)]

    for i in range(1, n + 1):
        for w in range(capacity + 1):
            # Don't take item i
            dp[i][w] = dp[i-1][w]

            # Take item i (if it fits)
            if weights[i-1] <= w:
                dp[i][w] = max(
                    dp[i][w],
                    dp[i-1][w - weights[i-1]] + values[i-1]
                )

    return dp[n][capacity]
```

#### 5. Longest Increasing Subsequence
```python
def lis(nums):
    """Length of longest increasing subsequence - O(n²)"""
    if not nums:
        return 0

    n = len(nums)
    dp = [1] * n  # Each element is LIS of length 1

    for i in range(1, n):
        for j in range(i):
            if nums[j] < nums[i]:
                dp[i] = max(dp[i], dp[j] + 1)

    return max(dp)

# O(n log n) version using binary search
def lis_optimized(nums):
    from bisect import bisect_left

    sub = []
    for num in nums:
        pos = bisect_left(sub, num)
        if pos == len(sub):
            sub.append(num)
        else:
            sub[pos] = num

    return len(sub)
```

---

## Graph Algorithms

### Breadth-First Search (BFS)
Explore level by level. Use for shortest path in unweighted graphs.

```python
from collections import deque

def bfs(graph, start):
    """O(V + E)"""
    visited = set([start])
    queue = deque([start])
    order = []

    while queue:
        vertex = queue.popleft()
        order.append(vertex)

        for neighbor in graph[vertex]:
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append(neighbor)

    return order

def shortest_path_bfs(graph, start, end):
    """Find shortest path in unweighted graph"""
    visited = set([start])
    queue = deque([(start, [start])])

    while queue:
        vertex, path = queue.popleft()

        if vertex == end:
            return path

        for neighbor in graph[vertex]:
            if neighbor not in visited:
                visited.add(neighbor)
                queue.append((neighbor, path + [neighbor]))

    return None  # No path exists
```

### Depth-First Search (DFS)
Explore as deep as possible before backtracking.

```python
def dfs_recursive(graph, start, visited=None):
    """O(V + E)"""
    if visited is None:
        visited = set()

    visited.add(start)
    result = [start]

    for neighbor in graph[start]:
        if neighbor not in visited:
            result.extend(dfs_recursive(graph, neighbor, visited))

    return result

def dfs_iterative(graph, start):
    """Using stack instead of recursion"""
    visited = set()
    stack = [start]
    result = []

    while stack:
        vertex = stack.pop()
        if vertex not in visited:
            visited.add(vertex)
            result.append(vertex)
            # Add neighbors in reverse for same order as recursive
            for neighbor in reversed(graph[vertex]):
                if neighbor not in visited:
                    stack.append(neighbor)

    return result
```

### Dijkstra's Algorithm
Shortest path in weighted graph with non-negative weights.

```python
import heapq

def dijkstra(graph, start):
    """
    O((V + E) log V) with min heap
    graph: {node: [(neighbor, weight), ...]}
    """
    distances = {node: float('inf') for node in graph}
    distances[start] = 0
    pq = [(0, start)]  # (distance, node)
    visited = set()

    while pq:
        current_dist, current = heapq.heappop(pq)

        if current in visited:
            continue
        visited.add(current)

        for neighbor, weight in graph[current]:
            distance = current_dist + weight

            if distance < distances[neighbor]:
                distances[neighbor] = distance
                heapq.heappush(pq, (distance, neighbor))

    return distances

# Example
graph = {
    'A': [('B', 1), ('C', 4)],
    'B': [('A', 1), ('C', 2), ('D', 5)],
    'C': [('A', 4), ('B', 2), ('D', 1)],
    'D': [('B', 5), ('C', 1)]
}
print(dijkstra(graph, 'A'))  # {'A': 0, 'B': 1, 'C': 3, 'D': 4}
```

### Topological Sort
Order vertices so all edges point forward. Only works on DAGs (Directed Acyclic Graphs).

```python
def topological_sort(graph):
    """Kahn's algorithm using BFS"""
    in_degree = {node: 0 for node in graph}

    # Calculate in-degrees
    for node in graph:
        for neighbor in graph[node]:
            in_degree[neighbor] += 1

    # Start with nodes that have no dependencies
    queue = deque([node for node in in_degree if in_degree[node] == 0])
    result = []

    while queue:
        node = queue.popleft()
        result.append(node)

        for neighbor in graph[node]:
            in_degree[neighbor] -= 1
            if in_degree[neighbor] == 0:
                queue.append(neighbor)

    # Check for cycle
    if len(result) != len(graph):
        return None  # Graph has a cycle

    return result
```

### Detect Cycle in Graph

```python
def has_cycle_undirected(graph):
    """Detect cycle in undirected graph using DFS"""
    visited = set()

    def dfs(node, parent):
        visited.add(node)
        for neighbor in graph[node]:
            if neighbor not in visited:
                if dfs(neighbor, node):
                    return True
            elif neighbor != parent:
                return True
        return False

    for node in graph:
        if node not in visited:
            if dfs(node, None):
                return True
    return False

def has_cycle_directed(graph):
    """Detect cycle in directed graph using colors"""
    WHITE, GRAY, BLACK = 0, 1, 2
    color = {node: WHITE for node in graph}

    def dfs(node):
        color[node] = GRAY

        for neighbor in graph[node]:
            if color[neighbor] == GRAY:  # Back edge found
                return True
            if color[neighbor] == WHITE and dfs(neighbor):
                return True

        color[node] = BLACK
        return False

    for node in graph:
        if color[node] == WHITE:
            if dfs(node):
                return True
    return False
```

---

## Two Pointers Technique

A pattern for solving array problems efficiently.

```python
def two_sum_sorted(arr, target):
    """Find pair that sums to target in sorted array - O(n)"""
    left, right = 0, len(arr) - 1

    while left < right:
        current_sum = arr[left] + arr[right]

        if current_sum == target:
            return (left, right)
        elif current_sum < target:
            left += 1
        else:
            right -= 1

    return None

def remove_duplicates(arr):
    """Remove duplicates from sorted array in-place - O(n)"""
    if len(arr) == 0:
        return 0

    write_ptr = 1

    for read_ptr in range(1, len(arr)):
        if arr[read_ptr] != arr[read_ptr - 1]:
            arr[write_ptr] = arr[read_ptr]
            write_ptr += 1

    return write_ptr

def is_palindrome(s):
    """Check if string is palindrome - O(n)"""
    left, right = 0, len(s) - 1

    while left < right:
        if s[left] != s[right]:
            return False
        left += 1
        right -= 1

    return True
```

---

## Sliding Window Technique

Maintain a window of elements for subarray/substring problems.

```python
def max_sum_subarray(arr, k):
    """Find maximum sum of subarray of size k - O(n)"""
    if len(arr) < k:
        return None

    # Calculate sum of first window
    window_sum = sum(arr[:k])
    max_sum = window_sum

    # Slide the window
    for i in range(k, len(arr)):
        window_sum += arr[i] - arr[i - k]
        max_sum = max(max_sum, window_sum)

    return max_sum

def longest_substring_k_distinct(s, k):
    """Longest substring with at most k distinct characters - O(n)"""
    char_count = {}
    max_length = 0
    left = 0

    for right in range(len(s)):
        char_count[s[right]] = char_count.get(s[right], 0) + 1

        while len(char_count) > k:
            char_count[s[left]] -= 1
            if char_count[s[left]] == 0:
                del char_count[s[left]]
            left += 1

        max_length = max(max_length, right - left + 1)

    return max_length
```

---

## Algorithm Selection Guide

```
Searching in sorted array?
    → Binary Search O(log n)

Searching in unsorted array?
    → Linear Search O(n) or sort first

Need shortest path (unweighted)?
    → BFS

Need shortest path (weighted)?
    → Dijkstra (non-negative weights)
    → Bellman-Ford (negative weights)

Need to explore all possibilities?
    → DFS / Backtracking

Overlapping subproblems?
    → Dynamic Programming

Make local optimal choices?
    → Greedy (verify it works!)

Processing array with constraints?
    → Two Pointers or Sliding Window
```

---

## Practice Problems by Pattern

### Binary Search
1. Search in rotated sorted array
2. Find peak element
3. Search a 2D matrix
4. Find minimum in rotated sorted array

### Two Pointers
1. Container with most water
2. 3Sum
3. Trapping rain water
4. Remove nth node from end of list

### Sliding Window
1. Longest substring without repeating characters
2. Minimum window substring
3. Maximum sum subarray of size k
4. Permutation in string

### Dynamic Programming
1. House robber
2. Longest palindromic substring
3. Word break
4. Edit distance

### Graph
1. Number of islands
2. Course schedule
3. Clone graph
4. Pacific Atlantic water flow

---

## Key Takeaways

1. **Know your patterns**: Most problems map to known algorithm patterns
2. **Start with brute force**: Then optimize using appropriate techniques
3. **Binary search requires sorted data**: But can be applied creatively
4. **BFS for shortest path**: In unweighted graphs
5. **DFS for exhaustive search**: Exploring all possibilities
6. **DP when you see overlapping subproblems**: Memoize or tabulate
7. **Two pointers for array pair problems**: Often O(n) instead of O(n²)
8. **Sliding window for subarray problems**: Maintain a dynamic window

---

*"An algorithm must be seen to be believed." — Donald Knuth*
